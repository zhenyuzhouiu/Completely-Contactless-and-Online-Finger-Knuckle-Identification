\section{Introduction}

Biometrics is the use of the human body's inherent physiological and behavioral characteristics for person identification. For the physiological characteristics, there are many human characteristics such as retina, iris, face, fingerprints and palm prints; as far as the behavioral characteristics, the available features include gait recognition, voice and other behavioral characteristics. These biometric characteristics are very popular areas of research, attracting many researchers, and have been applied in a wide range of industries. Among human physiological characteristics, finger knuckle, as an optional character, is easier to expose, convenient to collect, and the most importance is that it has been proven to be unique and stable \cite{kumar2014importance}. Therefore, it has also attracted many researchers devoted to it, related works including from the earlier \cite{kumar2009personal}, \cite{zhang2010online} to nowadays \cite{cheng2020deep}, \cite{tarawneh2022deepknuckle}. Contactless finger knuckle identification method, a research field of finger knuckle, is more user-friendly, security and more hygienic when compared to contact-based identification. Especially with today's epidemic, contactless identification method can efficiently inhibit the spread of the virus by preventing cross-contamination. It is due to the above-mentioned points that contactless finger knuckle have also gained a lot of attention. And due to the rapid development of neural networks, their powerful generalization capabilities and their great success in computer vision, researchers are increasingly inclined to use deep learning methods to solve the problem of contactless finger knuckle recognition.

Although the contactless finger knuckle identification offers a lot of convenience, there are two main problems at the contactless scenarios: one is the degradation of matching performance and the other is how to segment the finger knuckle region of interest (ROI) efficiently on the complexity background. Finger knuckle are more prone to deformation in the absence of fixation, which occurs not only in the 2D plane but also in the 3D plane, the presence of variations in ambient light, etc. These factors can cause crease texture of finger knuckle to vary considerably between individuals result in matching performance degradation. In the contactless scenario, finger knuckle are exposed to the interference of a complex background on captured contactless finger knuckle images or videos, and the position, size and number of finger knuckle appearing in each frame varies. This can make automatic finger knuckle detection and segmentation more difficult. If researchers can solve above two main problems, then the development and application of contactless finger knuckle identification will be very promising, and can enhance recognition performance with other biometrics identifiers.


\subsection{Related Work}
\textbf{Completely Contactless Finger Knuckle Identification}

Many early works on contactless finger knuckle identification, ranging from coding-based methods, subspace methods and texture analysis methods to 3D shape patterns based on 3D image reconstruction, and different methods have been used to achieve highly accurate recognition results. For the traditional recognition algorithms, there are generally two main types: one is holistic-based, and the other is local feature-based. The broad category can be divided into subspace and spectral representation methods for holistic-based \cite{yang2011finger}, \cite{neware2013finger}, \cite{meraoumia2011fusion}. Subspace methods are generally used for data dimensionality reduction and noise reduction \cite{zhang2006biometric}, such as the use of principal component analysis to reduce the dimensionality of multidimensional data. In contrast to spectral representation methods, image space transformation can be performed as well as image feature enhancement and correlation coefficients for feature extraction \cite{hennings2005verification}. For the processing of local information, there are many algorithms, including, for example, extracting information about the gradient of the image edges, obtaining the boundary points, using other edge extraction algorithms such as Hough change. For example, a 1D log-Gabor filter was used to extract the finger knuckles' features and for the matching phase, the hamming distance was used here for the matching score calculation since it is a local feature-based method \cite{meraoumiapersonal}. Alternatively, a 2D Gabor filter is used to extract the domain orientation information features of the finger knuckles, and an angular distance calculation is used to calculate the similarity between the different features for the matching score \cite{mehrotra1992gabor}. High recognition accuracy has been achieved for these matching algorithms, even up to $98.67\%$ \cite{yang2011finger}. Even early work considered application scenario using cell phones for finger knuckle recognition \cite{cheng2012contactless}, but for finger knuckle segmentation using fixed finger position in the center of the image is not very convenient for the user, for recognition phase log-Gabor is used for feature extraction, and Hamming distance is used for matching. Meanwhile, these paper \cite{kumar2019toward}, \cite{cheng2020deep}, \cite{kapoor2020completely} are the latest in research on the topic.

With the development of neural network, the generalization ability of feature extraction and feature matching method becomes more robust. Especially, from the oldest LeNet \cite{lecun1998gradient} to nowadays EfficientNetV2 \cite{tan2021efficientnetv2} model, the CNN models have achieved great success in computer vision tasks. Therefore, the CNN models also can be used on finger knuckle identification tasks to extract the robust features. Such as FKNet \cite{cheng2020deep} based on the ResNet-50 \cite{he2016deep}, it uses the FKNet to classify the subject by the 3D deep feature information of finger knuckle. There are even many that combine traditional methods with CNN models. For example \cite{kapoor2020completely}, it firstly uses initialized Gabor filters to extract features, and then uses CNN to learn the most important information, and the Gabor filter parameters can also update during backpropagation.

\textbf{Completely Contactless Fingers Detection and Segmentation}

For the efficiency of the matching algorithm, the accuracy of the region of interest of object is a factor that can determine the matching efficiency and accuracy. The size of the region of interest should be comparable to the size of the actual target object at the pixel level so that the size obtained after segmentation will not have redundant pixel information, which will reduce the pixel values to be computed for both the extraction and the subsequent matching sessions. 

As for the traditional method, their \cite{kumar2019toward}, \cite{cheng2012contactless} approach is to fix the finger knuckle position in the image when taking the finger knuckle data without complexity background, and then extract the edges of the object, and then extract the finger knuckle crease part. Most importantly, the traditional segmentation algorithm cannot correctly segment the finger knuckles in the presence of complex background interference, multiple finger knuckles in the same field of view, obscured finger knuckles or bent finger knuckles. It is difficult to use traditional object segmentation methods to automatically segment finger knuckles for applications such as in the wild. Models of neural network models for object detection have achieved great success, whether it is the sliding window detection algorithm, the 2-stage series of R-CNN models \cite{girshick2014rich}, \cite{girshick2015fast}, \cite{ren2015faster}, or the 1-stage YOLO series \cite{redmon2016you}, \cite{redmon2017yolo9000}, \cite{redmon2018yolov3}, \cite{bochkovskiy2020yolov4} and SSD models \cite{liu2016ssd} up to the current position, and even the anchor-free based object detection algorithm \cite{xin2021pafnet} as well. Each of these models has its advantages. For the 2-stage model, the object detection accuracy is guaranteed, the 1-stage based model is a speedup based on the positive accuracy, and the anchor-free is a further improvement in the detection speed. In this paper, the latest version of the YOLO model series, YOLOv5 \cite{YOLOv5}, is used as the network model for finger knuckle detection because the YOLO series is famous for its fast detection speed and high accuracy.


% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}[]
    \centering
    \begin{tabular}{ccccccc}
    \hline
    \multirow{2}{*}{Ref.}    & \multicolumn{3}{c|}{Deployment Scenario}                                                                                                                                                                                    & \multicolumn{3}{c}{Performance}                                                                                            \\ \cline{2-7} 
                             & \begin{tabular}[c]{@{}c@{}}Real-World\\ Knuckle\\ Detection\end{tabular} & \begin{tabular}[c]{@{}c@{}}Completely\\  Contactless\end{tabular} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Online\\ System\end{tabular}} & Database & EER             & \begin{tabular}[c]{@{}c@{}}Recognition\\ Rates\\ at\\ FAR=10\textasciicircum{}-4\end{tabular} \\ \hline
    \multirow{3}{*}{\cite{cheng2020deep}} & \multirow{3}{*}{No}                                                      & \multirow{3}{*}{No}                                               & \multirow{3}{*}{No}                                                          & D(1)     & 2.40\%          & 94.70\%                                                                                       \\
                             &                                                                          &                                                                   &                                                                              & A        & 3.90\%          & 86.60\%                                                                                       \\
                             &                                                                          &                                                                   &                                                                              & B        & 7.70\%          & 74.50\%                                                                                       \\ \hline
    \multirow{2}{*}{\cite{yang2018alpha}} & \multirow{2}{*}{No}                                                      & \multirow{2}{*}{No}                                               & \multirow{2}{*}{No}                                                          & A        & 2.72\%          & N/A                                                                                           \\
                             &                                                                          &                                                                   &                                                                              & E        & 0.66\%          & N/A                                                                                           \\ \hline
    {\cite{thapar2019fkimnet}}                  & No                                                                       & No                                                                & No                                                                           & A        & 3.97\%          & N/A                                                                                           \\ \hline
    \multirow{2}{*}{\cite{cheng2019contactless}} & \multirow{2}{*}{No}                                                      & \multirow{2}{*}{No}                                               & \multirow{2}{*}{No}                                                          & D        & 9.60\%          & $\sim$81.5\%                                                                                  \\
                             &                                                                          &                                                                   &                                                                              & D(2)     & 10.20\%         & $\sim$76.0\%                                                                                  \\ \hline
    \multirow{5}{*}{Ours}    & \multirow{5}{*}{Yes}                                                     & \multirow{5}{*}{Yes}                                              & \multirow{5}{*}{Yes}                                                         & C        & \textbf{2.21\%} & \textbf{$\sim$91\%}                                                                           \\
                             &                                                                          &                                                                   &                                                                              & B        & \textbf{1.33\%} & \textbf{$\sim$97\%}                                                                           \\
                             &                                                                          &                                                                   &                                                                              & D        & \textbf{1.05\%} & \textbf{$\sim$97.2\%}                                                                         \\
                             &                                                                          &                                                                   &                                                                              & D(2)     & \textbf{1.60\%} & \textbf{$\sim$96\%}                                                                           \\
                             &                                                                          &                                                                   &                                                                              & E        & \textbf{0.16\%} & \textbf{$\sim$99.9\%}                                                                         \\ \hline
    \end{tabular}
    \caption{Database description: A: The HKPolyU Contactless Finger Knuckle Images Database (Version 1.0) \cite{fingerknuckledbv1.0}; B: The HKPolyU Contactless Hand Dorsal Images Database \cite{ContactlessHnadDorsaldb}; C: The HKPolyU Contactless Finger Knuckle Images Database (Version 3.0) \cite{fingerknuckledbv3.0}; D: The HKPolyU 3D Finger Knuckle Images Database (D(1) 3D Images, D(2) 2D Images) \cite{3dfingerknuckle}; E: Tsinghua University Finger Vein and Finger Dorsal Texture Database \cite{thufvfdt3}.}
\end{table}


\textbf{Limitations and Challenges}

We have listed some methods on the finger knuckle matching in the above section. In terms of these traditional algorithms, they have a common problem that these algorithms have to keep changing their corresponding filters and even the corresponding detection parameters under different scenario \cite{zheng20163d}. Although, based on the deep learning, different model have already achieved high matching accuracy. But at present, \cite{cheng2020deep}, \cite{kapoor2020completely} and \cite{thapar2019fkimnet} do not think about finger knuckle rotate and shift problem, and even finger knuckle crease is very easily deformation on the contactless identification. These problems are very vital for the matching performance on the contactless method. There is a common problem for the classification neural networks, is the output classes is fixed during training, just like the FKNet \cite{cheng2020deep}, result in re-training when a new subject is added. Although, we can use the feature vector before the soft-max to calculate the similarity score when matching. 

As for the contactless finger knuckle detection and segmentation, \cite{kumar2016personal} finger knuckle ROI extraction method is hard to deploy on the contactless scenario, and as for the \cite{cheng2020deep}, it based on the Mask R-CNN to segmentation, but the speed is too slow for a real-time identification system. However, as for traditional segmentation algorithm, they cannot correctly segment the finger knuckles in the presence of complex background interference, multiple finger knuckles in the same field of view, obscured finger knuckles or bent finger knuckles. Meanwhile, due to lack of contactless finger knuckle images under complex background, all these neural networks cannot detect, and segment finger knuckle under complex background result in the problem that how to efficiently detection and segment the finger knuckle in the wild. At present, object detection and segmentation neural networks cannot simultaneously obtain the angle of finger knuckle and the segmentation with high precision. Especially, the angle of the finger knuckle is a vital factor for identification.


The \cite{cheng2012contactless} paper is the first attempt for designing a contactless finger knuckle identification system. But it cannot deal with the online or real-time scenario, because it is too slow to verification. And it cannot detect finger knuckle in the wild, the system requires users put their finger on fix region when take pictures, and only support verify one subject at the same time. When designing a contactless and online finger knuckle identification system, how to segment and recognize finger knuckle in real time or online under complex backgrounds and how to recognize multiple subjects at the same time from one image.



\subsection{Our Works}

As for the contactless finger knuckle identification, the most difficult problem is how to deal with the deformable crease texture of finger knuckle while with different angle. Although many methods have obtained good recognition accuracy, the finger knuckles are easily deformed under practical application scenarios, and the finger knuckle features will change accordingly. Thus, the matching accuracy will be degraded. Because of the above problems, there are corresponding studies to solve the finger knuckle deformation problem and provide new data sets and new methods \cite{kumar2019toward}. The paper \cite{kumar2019toward} first matches on two images for a selected fixed number $32*32$ of point pairs for coping with the deformation problem and then uses local feature descriptors on each point pair for matching. And at the RFNet \cite{liu2020contactless}, it shifts the feature maps for getting the minimal matching score to solving the palmprint shift problem. Meanwhile, on the person re-identification problem, the paper \cite{ahmed2015improved} uses the cross-input neighborhood differences module to calculate difference of one feature maps with another with more range to add robustness to positional differences, and the paper \cite{subramaniam2016deep} use the normalized correlation layer to achieve the similar problem. However, both of them just shift or calculate with more range but still on the horizontal and vertical direction, no one think about the rotation. Therefore, based on the Soft-Shifted Triplet Loss \cite{liu2020contactless}, we can also rotate to deal with more complex deformations.

Another problem is tow to automatically segment the finger knuckle region on the real time on the contactless and online finger knuckle identification. From the above contactless finger knuckle paper, they just use the traditional method to fix the finger knuckle position on an image or video. But just like I said before, the traditional method cannot solve the complexity phenomenon. And I have found that on the finger knuckle identification, it seems that no one use deep learning to detection and segmentation. A new finger knuckle object detection algorithm is used, which can automatically extract the region of interest of finger knuckle based on the YOLOv5 \cite{YOLOv5} model framework and integrates the angle prediction function. It is beneficial to the matching speed and accuracy of the matching algorithm and the automatic target segmentation using the object detection model to extract the ROI region of the major finger knuckle. The angle information of the finger knuckle can be obtained. If the algorithm of ROI segmentation is accurate enough and the accuracy of the rotation is also high enough, this will naturally improve the detection efficiency.

In conclusion, if we want to perform contactless finger knuckle recognition in a real-world scenario or an online scenario, the most critical problem comes from two aspects:how to match with high accuracy in real-world applications, and the second one is how to efficiently perform finger knuckle segmentation and correction. Therefore, our contribution can be summarized as below:
\begin{itemize}
    \item We design a new loss function to solve finger knuckle rotation and translation problem for getting more robust features, called RSIL loss function. After comparison, our RSIL can increase matching perform when compare to STTL \cite{liu2020contactless}. Even, our RSIL not only can be used on the finger knuckle identification, but also can be used on other biometric identification.
    \item Based on the YOLOv5, we use the CSL \cite{yang2020csl} to smooth angle classification, and add angular loss to prediction oriented bounding box for getting better quality segmented ROI. Meanwhile, we can use the angle information to normalize picture. From the segmented finger knuckle by YOLOv5-CSL, the matching perform can be increased when use these finger knuckle database offered. In comparison, such angular loss can significantly improve the performance for online finger knuckle identification.
    \item We design a cross-platform online finger knuckle identification system for completely contactless and online finger knuckle identification. The system can detect and match finger knuckle patterns from finger images acquired under complex background, using any ordinary smartphone or general laptop camera.
\end{itemize}

Section 3 will explain our designed RSIL loss function and prove that it can be differentiable by equation. Section 4 contains all experiments and corresponding results, including within database experiments and cross database experiments, and 2D finger knuckle and 3D finger knuckle. And also contain the ablation study with changing translation size and rotation size.
Section 5 introduces the finger knuckle detection model and how to implement the detection of finger knuckle angle information. Section 6 is to prove the online finger knuckle identification performance.