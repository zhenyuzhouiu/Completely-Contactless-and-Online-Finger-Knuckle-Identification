\section{Experiments and Results}
In this part, We will compare Translation and Rotation Triplet Loss (TRTL) and Shift Translation Triplet Loss \cite{liu2020contactless} performance on different finger knuckle dataset. Meanwhile, we will also compare with the state-of-the-art FKNet and EfficientNetV2. All the experiment will use the finger knuckle segmented by YOLOv5-CSL as training set and testing set. In the Online Contactless Finger Knuckle Identification section, we will compare the impact of the finger knuckle segmented by YOLOV5 and the finger knuckle provided by these databases on the performance. 

As for the DeConvRFNet, we just change the RFNet \cite{liu2020contactless} convolution layer with deformable convolution layer. Then as for DeConvRFNet and RFNet model, we pretrained on the Finger Knuckle V1 Database \cite{fingerknuckledbv1.0}.EfficientNetV2-S is the original classification model, we keep the same architecture and just change the FC layer of the head part with convolution layer for fitting TRTL and STTL to compose EfficientNetV2-S-STTL and EfficientNetV2-S-TRTL model. When trained these EfficientNetV2-S model, we use the pretrained weight on the ImageNet21K. As for the FKNet, we use the pretrained ResNet-50 weights.

Because the EfficientNetV2-S and FKNet is a classification model, I calculate MSE of two feature vectors of last layer of model as matching score. And as for the input data, I follow the same method of FKNet, each image will rotate from $-10^{\circ}$ to $10^{\circ}$ to increase the amount of training data.

\subsection{Model Complexity Analysis}
.........